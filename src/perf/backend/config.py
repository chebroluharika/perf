"""Ceph performance configs and use cases."""

ceph_configs = {
    "osd_memory_target": {
        "param": "osd_memory_target",
        "get_command": "ceph config get osd osd_memory_target",
        "set_command": "ceph config set osd osd_memory_target 8589934592",
        "expected": "8589934592",
        "comparator": "gt",
        "effect": "Reduces OSD read/write latency.",
        "positive": "'osd_memory_target' is already more than 8GB.",
        "suggest": "Increase 'osd_memory_target' to more than 8GB.",
    },
    "rbd_cache": {
        "param": "rbd_cache",
        "get_command": "ceph config get client rbd_cache",
        "set_command": "ceph config set client rbd_cache true",
        "expected": "true",
        "comparator": "eq",
        "effect": "Enables caching for faster reads.",
        "positive": "'rbd_cache' is already set.",
        "suggest": "Switch 'rbd_cache' to True.",
    },
    "osd_op_queue": {
        "param": "osd_op_queue",
        "get_command": "ceph config get osd osd_op_queue",
        "set_command": "ceph config set osd osd_op_queue wpq",
        "expected": ["wpq", "mclock_scheduler"],
        "comparator": "in",
        "effect": "enables better operation scheduling (Weighted Priority Queue (wpq))",
        "positive": "'osd_op_queue' is already using WPQ.",
        "suggest": "Enable Weighted Priority Queue (wpq) as scheduler.",
    },
    "osd_max_write_size": {
        "param": "osd_max_write_size",
        "get_command": "ceph config get osd osd_max_write_size",
        "set_command": "ceph config set osd osd_max_write_size 128MB",
        "expected": "268435456",
        "comparator": "gt",
        "effect": "Defines the maximum write size per operation, optimizing large writes.",
        "positive": "osd_max_write_size is set optimally.",
        "suggest": "Increase osd_max_write_size for better large I/O performance.",
    },
    "osd_recovery_max_active": {
        "param": "osd_recovery_max_active",
        "get_command": "ceph config get osd osd_recovery_max_active",
        "set_command": "ceph config set osd osd_recovery_max_active 4",
        "expected": "4",
        "comparator": "gt",
        "effect": "Controls the maximum concurrent recovery operations per OSD.",
        "positive": "osd_recovery_max_active is set optimally.",
        "suggest": "Increase for faster recovery speed.",
    },
    "rbd_cache_size": {
        "param": "rbd_cache_size",
        "get_command": "ceph config get client rbd_cache_size",
        "set_command": "ceph config set client rbd_cache_size 256MB",
        "expected": "268435456",
        "comparator": "gt",
        "effect": "Defines the maximum memory allocated to the RADOS block device cache.",
        "positive": "rbd_cache_max_size is set optimally.",
        "suggest": "Increase to speed up repeated disk access, especially for frequent reads.",
    },
    "bluestore_min_alloc_size_ssd": {
        "param": "bluestore_min_alloc_size_ssd",
        "get_command": "ceph config get osd bluestore_min_alloc_size_ssd",
        "set_command": "ceph config set osd bluestore_min_alloc_size_ssd 64K",
        "expected": "67108864",
        "comparator": "gt",
        "effect": "Defines the minimum allocation size for SSD storage backend.",
        "positive": "bluestore_min_alloc_size_ssd is set optimally.",
        "suggest": "Increase to reduce fragmentation for VM disk images.",
    },
    "bluestore_min_alloc_size_hdd": {
        "param": "bluestore_min_alloc_size_hdd",
        "get_command": "ceph config get osd bluestore_min_alloc_size_hdd",
        "set_command": "ceph config set osd bluestore_min_alloc_size_hdd 64K",
        "expected": "67108864",
        "comparator": "gt",
        "effect": "Defines the minimum allocation size for HDD storage backend.",
        "positive": "bluestore_min_alloc_size_hdd is set optimally.",
        "suggest": "Increase to optimize for large writes.",
    },
    "osd_max_backfills": {
        "param": "osd_max_backfills",
        "get_command": "ceph config get osd osd_max_backfills",
        "set_command": "ceph config set osd osd_max_backfills 4",
        "expected": "4",
        "comparator": "gt",
        "effect": "Controls the number of simultaneous backfill operations per OSD.",
        "positive": "osd_max_backfills is set optimally.",
        "suggest": "Increase for faster data recovery with minimal impact.",
    },
    "ms_dispatch_throttle_bytes": {
        "param": "osd_op_threads",
        "get_command": "ceph config get osd ms_dispatch_throttle_bytes",
        "set_command": "ceph config set osd ms_dispatch_throttle_bytes 100MB",
        "expected": "104857600",
        "comparator": "gt",
        "effect": "Defines the number of worker threads handling client I/O operations.",
        "positive": "osd_op_threads is set optimally.",
        "suggest": "Adjust based on CPU resources to improve OSD parallelism.",
    },
    "bluestore_cache_autotune": {
        "param": "bluestore_cache_autotune",
        "get_command": "ceph config get osd bluestore_cache_autotune",
        "set_command": "ceph config set osd bluestore_cache_autotune true",
        "expected": "true",
        "comparator": "eq",
        "effect": "Enables automatic adjustment of BlueStore cache size.",
        "positive": "bluestore_cache_autotune is enabled for dynamic tuning.",
        "suggest": "Enable to allow Ceph to optimize memory usage dynamically.",
    },
    "rgw_max_concurrent_requests": {
        "param": "rgw_max_concurrent_requests",
        "get_command": "ceph config get client.rgw rgw_max_concurrent_requests",
        "set_command": "ceph config set client.rgw rgw_max_concurrent_requests 2048",
        "expected": "1024",
        "comparator": "gt",
        "effect": "Maximum number of concurrent HTTP requests",
        "positive": "rgw_max_concurrent_requests is set optimally.",
        "suggest": "Increase to optimize memory usage under heavy load.",
    },
    "rgw_gc_max_concurrent_io": {
        "param": "rgw_gc_max_concurrent_io",
        "get_command": "ceph config get client.rgw rgw_gc_max_concurrent_io",
        "set_command": "ceph config set client.rgw rgw_gc_max_concurrent_io 20",
        "expected": "10",
        "comparator": "gt",
        "effect": "Controls the number of concurrent threads for RGW garbage collection",
        "positive": "rgw_gc_max_concurrent_io is set optimally.",
        "suggest": "Increase to optimize concurrent delete operations and to avoid throttling.",
    },
    "rgw_cache_lru_size": {
        "param": "rgw_cache_lru_size",
        "get_command": "ceph config get client.rgw rgw_cache_lru_size",
        "set_command": "ceph config set client.rgw rgw_cache_lru_size 25000",
        "expected": "25000",
        "comparator": "eq",
        "effect": "Maximum number of concurrent HTTP requests",
        "positive": "rgw_cache_lru_size is set optimally.",
        "suggest": "Increase to reduce the memory usage.",
    },
}

USE_CASES = {
    "LOW_LATENCY_DBS_WORKLOAD": ["osd_memory_target", "rbd_cache", "osd_op_queue"],
    "HIGH_THROUGHPUT_WORKLOADS": [
        "osd_max_write_size",
        "osd_recovery_max_active",
        "bluestore_min_alloc_size_hdd",
    ],
    "VIRTUAL_MACHINE_STORAGE": [
        "rbd_cache",
        "rbd_cache_size",
        "bluestore_min_alloc_size_ssd",
    ],
    "BIG_DATA_ANALYTICS": [
        "osd_max_backfills",
        "ms_dispatch_throttle_bytes",
        "bluestore_cache_autotune",
    ],
    "OBJECT_WORKLOADS": [
        "rgw_max_concurrent_requests",
        "rgw_gc_max_concurrent_io",
        "rgw_cache_lru_size",
    ],
}
